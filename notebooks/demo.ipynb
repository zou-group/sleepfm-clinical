{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing Details\n",
    "\n",
    "\n",
    "Before running this notebook, please preprocess your PSG files using the scripts provided in `sleepfm/preprocessing`. Note that PSG recordings may contain different sets of channels across datasets. The predefined channel–modality mappings used in this project are specified in `sleepfm/configs/channel_groups.json`.\n",
    "\n",
    "Although we have attempted to make this mapping as comprehensive as possible, we strongly recommend reviewing the channels present in your specific PSG data. In consultation with domain experts, you should group any additional or dataset-specific channels into the appropriate modality categories and update `channel_groups.json` accordingly. This step is critical to ensure that all channels are correctly aligned with their intended modalities during preprocessing and downstream modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "import os\n",
    "import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../sleepfm\")\n",
    "from preprocessing.preprocessing import EDFToHDF5Converter\n",
    "from models.dataset import SetTransformerDataset, collate_fn\n",
    "from models.models import SetTransformer, SleepEventLSTMClassifier, DiagnosisFinetuneFullLSTMCOXPHWithDemo\n",
    "import h5py\n",
    "from utils import load_config, load_data, save_data, count_parameters\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 0: Preprocessing EDF files\n",
    "\n",
    "Note: This is just a demo notebook that preprocesses a single, specific file. run `sleepfm/preprocessing/preprocessing.sh` with appropriate folders to generate multiple preprocessed files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-05 14:31:05.421\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpreprocessing.preprocessing\u001b[0m:\u001b[36mread_edf\u001b[0m:\u001b[36m146\u001b[0m - \u001b[1mreading edf\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from /oak/stanford/groups/mignot/psg/SSC_Stanford/all/SSC_2004_1006994706.EDF...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 8937983  =      0.000 ... 34913.996 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-05 14:31:35.082\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpreprocessing.preprocessing\u001b[0m:\u001b[36mresample_signals\u001b[0m:\u001b[36m184\u001b[0m - \u001b[1mresampling signals\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering signal\n",
      "Filtering signal\n",
      "Filtering signal\n",
      "Filtering signal\n",
      "Filtering signal\n",
      "Filtering signal\n",
      "Filtering signal\n",
      "Filtering signal\n",
      "Filtering signal\n",
      "Filtering signal\n",
      "Filtering signal\n",
      "Filtering signal\n",
      "Filtering signal\n",
      "Filtering signal\n",
      "Filtering signal\n",
      "Filtering signal\n",
      "Filtering signal\n",
      "Filtering signal\n",
      "Filtering signal\n",
      "Filtering signal\n",
      "Filtering signal\n",
      "Filtering signal\n",
      "Filtering signal\n",
      "Filtering signal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-05 14:31:46.826\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpreprocessing.preprocessing\u001b[0m:\u001b[36msave_to_hdf5\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1msaving hdf5\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "root_dir = \"/edf_root\"      # dummy root not used for a single file conversion\n",
    "target_dir = \"/note\"    # dummy target not used for a single file conversion\n",
    "\n",
    "#edf_path = \"/path/to/demo_psg.edf\"\n",
    "# edf_path = \"/oak/stanford/groups/mignot/psg/SSC_Stanford/ssc/2015/SSC_2015_0002.edf\"\n",
    "edf_path = \"/oak/stanford/groups/mignot/psg/SSC_Stanford/all/SSC_2004_1006994706.EDF\"\n",
    "hdf5_path = \"demo_psg.hdf5\"\n",
    "\n",
    "converter = EDFToHDF5Converter(\n",
    "    root_dir=root_dir,\n",
    "    target_dir=target_dir,\n",
    "    resample_rate=128\n",
    ")\n",
    "\n",
    "# run for single file conversion\n",
    "converter.convert(edf_path, hdf5_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1: Generating embeddings from SleepFM pretrained model\n",
    "\n",
    "Here we show generating embedding for 1 demno PSG. To see full script, please check `sleepfm/pipeline/generate_embeddings.py`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../sleepfm/checkpoints/model_base\"\n",
    "channel_groups_path = \"../sleepfm/configs/channel_groups.json\"\n",
    "config_path = os.path.join(model_path, \"config.json\")\n",
    "\n",
    "config = load_config(config_path)\n",
    "channel_groups = load_data(channel_groups_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "modality_types = config[\"modality_types\"]\n",
    "in_channels = config[\"in_channels\"]\n",
    "patch_size = config[\"patch_size\"]\n",
    "embed_dim = config[\"embed_dim\"]\n",
    "num_heads = config[\"num_heads\"]\n",
    "num_layers = config[\"num_layers\"]\n",
    "pooling_head = config[\"pooling_head\"]\n",
    "dropout = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 4.44 million\n",
      "Number of layers: 93\n"
     ]
    }
   ],
   "source": [
    "model_class = getattr(sys.modules[__name__], config['model'])\n",
    "model = model_class(in_channels, patch_size, embed_dim, num_heads, num_layers, pooling_head=pooling_head, dropout=dropout)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "if device.type == \"cuda\":\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "model.to(device)\n",
    "total_layers, total_params = count_parameters(model)\n",
    "print(f'Trainable parameters: {total_params / 1e6:.2f} million')\n",
    "print(f'Number of layers: {total_layers}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): SetTransformer(\n",
       "    (patch_embedding): Tokenizer(\n",
       "      (tokenizer): Sequential(\n",
       "        (0): Conv1d(1, 4, kernel_size=(5,), stride=(2,), padding=(2,))\n",
       "        (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ELU(alpha=1.0)\n",
       "        (3): LayerNorm((4, 320), eps=1e-05, elementwise_affine=True)\n",
       "        (4): Conv1d(4, 8, kernel_size=(5,), stride=(2,), padding=(2,))\n",
       "        (5): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (6): ELU(alpha=1.0)\n",
       "        (7): LayerNorm((8, 160), eps=1e-05, elementwise_affine=True)\n",
       "        (8): Conv1d(8, 16, kernel_size=(5,), stride=(2,), padding=(2,))\n",
       "        (9): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (10): ELU(alpha=1.0)\n",
       "        (11): LayerNorm((16, 80), eps=1e-05, elementwise_affine=True)\n",
       "        (12): Conv1d(16, 32, kernel_size=(5,), stride=(2,), padding=(2,))\n",
       "        (13): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (14): ELU(alpha=1.0)\n",
       "        (15): LayerNorm((32, 40), eps=1e-05, elementwise_affine=True)\n",
       "        (16): Conv1d(32, 64, kernel_size=(5,), stride=(2,), padding=(2,))\n",
       "        (17): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (18): ELU(alpha=1.0)\n",
       "        (19): LayerNorm((64, 20), eps=1e-05, elementwise_affine=True)\n",
       "        (20): Conv1d(64, 128, kernel_size=(5,), stride=(2,), padding=(2,))\n",
       "        (21): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (22): ELU(alpha=1.0)\n",
       "        (23): LayerNorm((128, 10), eps=1e-05, elementwise_affine=True)\n",
       "        (24): AdaptiveAvgPool1d(output_size=1)\n",
       "        (25): Flatten(start_dim=1, end_dim=-1)\n",
       "        (26): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (spatial_pooling): AttentionPooling(\n",
       "      (transformer_layer): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (positional_encoding): PositionalEncoding()\n",
       "    (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer_encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (temporal_pooling): AttentionPooling(\n",
       "      (transformer_layer): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(os.path.join(model_path, \"best.pt\"))\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'seed': 42,\n",
       " 'model': 'SetTransformer',\n",
       " 'in_channels': 1,\n",
       " 'batch_size': 128,\n",
       " 'epochs': 1,\n",
       " 'lr': 0.001,\n",
       " 'lr_step_period': 2,\n",
       " 'gamma': 0.1,\n",
       " 'temperature': 0.0,\n",
       " 'momentum': 0.9,\n",
       " 'num_workers': 16,\n",
       " 'embed_dim': 128,\n",
       " 'num_heads': 8,\n",
       " 'num_layers': 6,\n",
       " 'pooling_head': 8,\n",
       " 'dropout': 0.3,\n",
       " 'split_path': 'path_to_/dataset_split.json',\n",
       " 'save_path': 'path_to_/models',\n",
       " 'weight_decay': 0.0,\n",
       " 'mode': 'leave_one_out',\n",
       " 'save_iter': 5000,\n",
       " 'eval_iter': 5000,\n",
       " 'log_interval': 100,\n",
       " 'use_wandb': True,\n",
       " 'BAS_CHANNELS': 10,\n",
       " 'RESP_CHANNELS': 7,\n",
       " 'EKG_CHANNELS': 2,\n",
       " 'EMG_CHANNELS': 4,\n",
       " 'max_files': None,\n",
       " 'val_size': 100,\n",
       " 'sampling_duration': 5,\n",
       " 'sampling_freq': 128,\n",
       " 'patch_size': 640,\n",
       " 'modality_types': ['BAS', 'RESP', 'EKG', 'EMG']}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing files: 100%|██████████| 1/1 [00:00<00:00, 18.69it/s]\n"
     ]
    }
   ],
   "source": [
    "hdf5_paths = [\"demo_psg.hdf5\"]\n",
    "dataset = SetTransformerDataset(config, channel_groups, hdf5_paths=hdf5_paths, split=\"test\")\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, \n",
    "                                            batch_size=16, \n",
    "                                            num_workers=1, \n",
    "                                            shuffle=False, \n",
    "                                            collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = \"demo_emb\"\n",
    "output_5min_agg = f\"demo_5min_agg_emb\"\n",
    "os.makedirs(output, exist_ok=True)\n",
    "os.makedirs(output_5min_agg, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]/home/groups/jamesz/rthapa84/anaconda3/envs/sleepfm_clinical/lib/python3.10/site-packages/torch/nn/modules/transformer.py:562: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._transformer_encoder_layer_fwd(\n",
      "100%|██████████| 8/8 [00:16<00:00,  2.05s/it]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    with tqdm.tqdm(total=len(dataloader)) as pbar:\n",
    "        for batch in dataloader:\n",
    "            batch_data, mask_list, file_paths, dset_names_list, chunk_starts = batch\n",
    "            (bas, resp, ekg, emg) = batch_data\n",
    "            (mask_bas, mask_resp, mask_ekg, mask_emg) = mask_list\n",
    "\n",
    "            bas = bas.to(device, dtype=torch.float)\n",
    "            resp = resp.to(device, dtype=torch.float)\n",
    "            ekg = ekg.to(device, dtype=torch.float)\n",
    "            emg = emg.to(device, dtype=torch.float)\n",
    "\n",
    "            mask_bas = mask_bas.to(device, dtype=torch.bool)\n",
    "            mask_resp = mask_resp.to(device, dtype=torch.bool)\n",
    "            mask_ekg = mask_ekg.to(device, dtype=torch.bool)\n",
    "            mask_emg = mask_emg.to(device, dtype=torch.bool)\n",
    "\n",
    "            embeddings = [\n",
    "                model(bas, mask_bas),\n",
    "                model(resp, mask_resp),\n",
    "                model(ekg, mask_ekg),\n",
    "                model(emg, mask_emg),\n",
    "            ]\n",
    "\n",
    "            # Model gives two kinds of embeddings. Granular 5 second-level embeddings and aggregated 5 minute-level embeddings. We save both of them below. \n",
    "\n",
    "            embeddings_new = [e[0].unsqueeze(1) for e in embeddings]\n",
    "\n",
    "            for i in range(len(file_paths)):\n",
    "                file_path = file_paths[i]\n",
    "                chunk_start = chunk_starts[i]\n",
    "                subject_id = os.path.basename(file_path).split('.')[0]\n",
    "                output_path = os.path.join(output_5min_agg, f\"{subject_id}.hdf5\")\n",
    "\n",
    "                with h5py.File(output_path, 'a') as hdf5_file:\n",
    "                    for modality_idx, modality_type in enumerate(config[\"modality_types\"]):\n",
    "                        if modality_type in hdf5_file:\n",
    "                            dset = hdf5_file[modality_type]\n",
    "                            chunk_start_correct = chunk_start // (embed_dim * 5 * 60)\n",
    "                            chunk_end = chunk_start_correct + embeddings_new[modality_idx][i].shape[0]\n",
    "                            if dset.shape[0] < chunk_end:\n",
    "                                dset.resize((chunk_end,) + embeddings_new[modality_idx][i].shape[1:])\n",
    "                            dset[chunk_start_correct:chunk_end] = embeddings_new[modality_idx][i].cpu().numpy()\n",
    "                        else:\n",
    "                            hdf5_file.create_dataset(modality_type, data=embeddings_new[modality_idx][i].cpu().numpy(), chunks=(embed_dim,) + embeddings_new[modality_idx][i].shape[1:], maxshape=(None,) + embeddings_new[modality_idx][i].shape[1:])\n",
    "\n",
    "            embeddings_new = [e[1] for e in embeddings]\n",
    "\n",
    "            for i in range(len(file_paths)):\n",
    "                file_path = file_paths[i]\n",
    "                chunk_start = chunk_starts[i]\n",
    "                subject_id = os.path.basename(file_path).split('.')[0]\n",
    "                output_path = os.path.join(output, f\"{subject_id}.hdf5\")\n",
    "\n",
    "                with h5py.File(output_path, 'a') as hdf5_file:\n",
    "                    for modality_idx, modality_type in enumerate(config[\"modality_types\"]):\n",
    "                        if modality_type in hdf5_file:\n",
    "                            dset = hdf5_file[modality_type]\n",
    "                            chunk_start_correct = chunk_start // (embed_dim * 5)\n",
    "                            chunk_end = chunk_start_correct + embeddings_new[modality_idx][i].shape[0]\n",
    "                            if dset.shape[0] < chunk_end:\n",
    "                                dset.resize((chunk_end,) + embeddings_new[modality_idx][i].shape[1:])\n",
    "                            dset[chunk_start_correct:chunk_end] = embeddings_new[modality_idx][i].cpu().numpy()\n",
    "                        else:\n",
    "                            hdf5_file.create_dataset(modality_type, data=embeddings_new[modality_idx][i].cpu().numpy(), chunks=(embed_dim,) + embeddings_new[modality_idx][i].shape[1:], maxshape=(None,) + embeddings_new[modality_idx][i].shape[1:])\n",
    "            pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2: Sleep Staging\n",
    "\n",
    "Note that below, we are using our finetuned sleep staging model. It is always a good idea to finetune our model on your specific data, even if you only have a handful of sample, so that the model can adapt to your specific data distribution. Script to finetune your sleep staging model head is given in `sleepfm/pipeline/finetune_sleep_staging.py`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_staging_model_path = \"../sleepfm/checkpoints/model_sleep_staging\"\n",
    "sleep_staging_config = load_data(os.path.join(sleep_staging_model_path, \"config.json\"))\n",
    "\n",
    "sleep_staging_model_params = sleep_staging_config['model_params']\n",
    "sleep_staging_model_class = getattr(sys.modules[__name__], sleep_staging_config['model'])\n",
    "\n",
    "sleep_staging_model = sleep_staging_model_class(**sleep_staging_model_params).to(device)\n",
    "sleep_staging_model_name = type(sleep_staging_model).__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 GPUs\n"
     ]
    }
   ],
   "source": [
    "sleep_staging_model = nn.DataParallel(sleep_staging_model)\n",
    "print(f\"Using {torch.cuda.device_count()} GPUs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized: SleepEventLSTMClassifier\n",
      "Trainable parameters: 1.19 million\n",
      "Number of layers: 20\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model initialized: {sleep_staging_model_name}\")\n",
    "total_layers, total_params = count_parameters(sleep_staging_model)\n",
    "print(f'Trainable parameters: {total_params / 1e6:.2f} million')\n",
    "print(f'Number of layers: {total_layers}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sleep_staging_checkpoint_path = os.path.join(sleep_staging_model_path, \"best.pth\")\n",
    "sleep_staging_checkpoint = torch.load(sleep_staging_checkpoint_path)\n",
    "sleep_staging_model.load_state_dict(sleep_staging_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some helper functions for loading data for sleep staging. You can find similar functions within `sleepfm/models/dataset.py`. You may need to modify it slightly based on your usecase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this function if you do not want any dependency on gold labels and just want to get logprobs\n",
    "\n",
    "# class SleepEventClassificationDataset(Dataset):\n",
    "#     def __init__(self,\n",
    "#                  config,\n",
    "#                  channel_groups,\n",
    "#                  hdf5_paths=None,\n",
    "#                  split=\"train\"):\n",
    "\n",
    "#         self.config = config\n",
    "#         self.max_channels = self.config[\"max_channels\"]\n",
    "#         self.context = int(self.config[\"context\"])\n",
    "#         self.channel_like = self.config[\"channel_like\"]\n",
    "\n",
    "#         # ---- Resolve HDF5 paths (simple rule) ----\n",
    "#         # If hdf5_paths is provided, use it. Otherwise, load from config[\"split_path\"].\n",
    "#         if hdf5_paths:\n",
    "#             hdf5_paths = [p for p in hdf5_paths if os.path.exists(p)]\n",
    "#         else:\n",
    "#             data_path = config[\"data_path\"]\n",
    "#             split_paths = load_data(config[\"split_path\"])[split]\n",
    "#             hdf5_paths = []\n",
    "#             for rel_path in split_paths:\n",
    "#                 abs_path = os.path.join(data_path, rel_path)\n",
    "#                 if os.path.exists(abs_path):\n",
    "#                     hdf5_paths.append(abs_path)\n",
    "\n",
    "#         # Optional truncation\n",
    "#         if config.get(\"max_files\"):\n",
    "#             hdf5_paths = hdf5_paths[:config[\"max_files\"]]\n",
    "\n",
    "#         # ---- Build index map ----\n",
    "#         if self.context == -1:\n",
    "#             self.index_map = [(path, -1) for path in hdf5_paths]\n",
    "#         else:\n",
    "#             self.index_map = []\n",
    "#             loop = tqdm(hdf5_paths, total=len(hdf5_paths), desc=f\"Indexing {split} data\")\n",
    "#             for hdf5_file_path in loop:\n",
    "#                 try:\n",
    "#                     with h5py.File(hdf5_file_path, \"r\") as file:\n",
    "#                         dset_names = list(file.keys())\n",
    "#                         if len(dset_names) == 0:\n",
    "#                             continue\n",
    "#                         # Use the first dataset to infer length (matches your original logic)\n",
    "#                         dset0 = dset_names[0]\n",
    "#                         dataset_length = file[dset0].shape[0]\n",
    "#                         for i in range(0, dataset_length, self.context):\n",
    "#                             self.index_map.append((hdf5_file_path, i))\n",
    "#                 except OSError:\n",
    "#                     # Corrupt/unreadable file; skip\n",
    "#                     continue\n",
    "\n",
    "#         print(f\"Number of files in {split} set: {len(hdf5_paths)}\")\n",
    "#         print(f\"Number of segments to be processed in {split} set: {len(self.index_map)}\")\n",
    "\n",
    "#         self.total_len = len(self.index_map)\n",
    "#         self.max_seq_len = config[\"model_params\"][\"max_seq_length\"]\n",
    "\n",
    "#         if self.total_len == 0:\n",
    "#             raise ValueError(f\"No valid samples found for split='{split}'. Check paths/config.\")\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.total_len\n",
    "\n",
    "#     def get_index_map(self):\n",
    "#         return self.index_map\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         hdf5_path, start_index = self.index_map[idx]\n",
    "\n",
    "#         x_data = []\n",
    "#         try:\n",
    "#             with h5py.File(hdf5_path, \"r\") as hf:\n",
    "#                 dset_names = list(hf.keys())\n",
    "#                 for dataset_name in dset_names:\n",
    "#                     if dataset_name in self.channel_like:\n",
    "#                         if self.context == -1:\n",
    "#                             x_data.append(hf[dataset_name][:])\n",
    "#                         else:\n",
    "#                             x_data_in = hf[dataset_name][start_index:start_index + self.context]\n",
    "#                             x_data.append(x_data_in)\n",
    "#         except OSError:\n",
    "#             # If file can't be read, skip to next example\n",
    "#             return self.__getitem__((idx + 1) % self.total_len)\n",
    "\n",
    "#         if not x_data:\n",
    "#             # Skip this data point if x_data is empty\n",
    "#             return self.__getitem__((idx + 1) % self.total_len)\n",
    "\n",
    "#         # Convert list -> array -> tensor (keeps your original behavior)\n",
    "#         x_data = np.array(x_data)\n",
    "#         x_data = torch.tensor(x_data, dtype=torch.float32)\n",
    "\n",
    "#         return x_data, self.max_channels, self.max_seq_len, hdf5_path\n",
    "\n",
    "\n",
    "# def sleep_event_finetune_full_collate_fn(batch):\n",
    "#     x_data, max_channels_list, max_seq_len_list, hdf5_path_list = zip(*batch)\n",
    "\n",
    "#     num_channels = max(max_channels_list)\n",
    "\n",
    "#     max_seq_len_temp = max([item.size(1) for item in x_data])\n",
    "\n",
    "#     # Determine the max sequence length for padding\n",
    "#     if max_seq_len_list[0] is None:\n",
    "#         max_seq_len = max_seq_len_temp\n",
    "#     else:\n",
    "#         max_seq_len = min(max_seq_len_temp, max_seq_len_list[0])\n",
    "\n",
    "#     padded_x_data = []\n",
    "#     padded_mask = []\n",
    "\n",
    "#     for x_item in x_data:\n",
    "#         # Get the shape of x_item\n",
    "#         c, s, e = x_item.size()\n",
    "#         c = min(c, num_channels)\n",
    "#         s = min(s, max_seq_len)  # Ensure the sequence length doesn't exceed max_seq_len\n",
    "\n",
    "#         # Create a padded tensor and a mask tensor for x_data\n",
    "#         padded_x_item = torch.zeros((num_channels, max_seq_len, e))\n",
    "#         mask = torch.ones((num_channels, max_seq_len))\n",
    "\n",
    "#         # Copy the actual data to the padded tensor and set the mask for real data\n",
    "#         padded_x_item[:c, :s, :e] = x_item[:c, :s, :e]\n",
    "#         mask[:c, :s] = 0  # 0 for real data, 1 for padding\n",
    "\n",
    "#         padded_x_data.append(padded_x_item)\n",
    "#         padded_mask.append(mask)\n",
    "\n",
    "#     # Stack all tensors into a batch\n",
    "#     x_data = torch.stack(padded_x_data)\n",
    "#     padded_mask = torch.stack(padded_mask)\n",
    "    \n",
    "#     return x_data, padded_mask, hdf5_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SleepEventClassificationDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config,\n",
    "        channel_groups,\n",
    "        hdf5_paths,\n",
    "        label_files,\n",
    "        split=\"train\",\n",
    "    ):\n",
    "        self.config = config\n",
    "        self.max_channels = self.config[\"max_channels\"]\n",
    "        self.context = int(self.config[\"context\"])\n",
    "        self.channel_like = self.config[\"channel_like\"]\n",
    "\n",
    "        self.max_seq_len = config[\"model_params\"][\"max_seq_length\"]\n",
    "\n",
    "        # --- Build label lookup: {study_id: label_csv_path} ---\n",
    "        # study_id = filename without extension, e.g. \"SSC_12345\"\n",
    "        labels_dict = {\n",
    "            os.path.basename(p).rsplit(\".\", 1)[0]: p\n",
    "            for p in label_files\n",
    "            if os.path.exists(p)\n",
    "        }\n",
    "\n",
    "        # --- Filter to HDF5s that exist and have a matching label file ---\n",
    "        hdf5_paths = [p for p in hdf5_paths if os.path.exists(p)]\n",
    "        hdf5_paths = [\n",
    "            p for p in hdf5_paths\n",
    "            if os.path.basename(p).rsplit(\".\", 1)[0] in labels_dict\n",
    "        ]\n",
    "\n",
    "        if config.get(\"max_files\"):\n",
    "            hdf5_paths = hdf5_paths[: config[\"max_files\"]]\n",
    "\n",
    "        self.hdf5_paths = hdf5_paths\n",
    "        self.labels_dict = labels_dict\n",
    "\n",
    "        # --- Build index map ---\n",
    "        # Each item is (hdf5_path, label_path, start_index)\n",
    "        if self.context == -1:\n",
    "            self.index_map = [\n",
    "                (p, labels_dict[os.path.basename(p).rsplit(\".\", 1)[0]], -1)\n",
    "                for p in self.hdf5_paths\n",
    "            ]\n",
    "        else:\n",
    "            self.index_map = []\n",
    "            loop = tqdm(self.hdf5_paths, total=len(self.hdf5_paths), desc=f\"Indexing {split} data\")\n",
    "            for hdf5_file_path in loop:\n",
    "                file_prefix = os.path.basename(hdf5_file_path).rsplit(\".\", 1)[0]\n",
    "                label_path = labels_dict[file_prefix]\n",
    "\n",
    "                with h5py.File(hdf5_file_path, \"r\") as hf:\n",
    "                    dset_names = list(hf.keys())\n",
    "                    if len(dset_names) == 0:\n",
    "                        continue\n",
    "\n",
    "                    # Use first dataset to define length (same as your original behavior)\n",
    "                    first_name = dset_names[0]\n",
    "                    dataset_length = hf[first_name].shape[0]\n",
    "\n",
    "                for i in range(0, dataset_length, self.context):\n",
    "                    self.index_map.append((hdf5_file_path, label_path, i))\n",
    "\n",
    "        # If you have logger, keep; otherwise you can remove these.\n",
    "        # logger.info(f\"Number of files in {split} set: {len(self.hdf5_paths)}\")\n",
    "        # logger.info(f\"Number of files to be processed in {split} set: {len(self.index_map)}\")\n",
    "\n",
    "        self.total_len = len(self.index_map)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_len\n",
    "\n",
    "    def get_index_map(self):\n",
    "        return self.index_map\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        hdf5_path, label_path, start_index = self.index_map[idx]\n",
    "\n",
    "        labels_df = pd.read_csv(label_path)\n",
    "        labels_df[\"StageNumber\"] = labels_df[\"StageNumber\"].replace(-1, 0)\n",
    "\n",
    "        y_data = labels_df[\"StageNumber\"].to_numpy()\n",
    "        if self.context != -1:\n",
    "            y_data = y_data[start_index : start_index + self.context]\n",
    "\n",
    "        x_data = []\n",
    "        with h5py.File(hdf5_path, \"r\") as hf:\n",
    "            dset_names = list(hf.keys())\n",
    "\n",
    "            for dataset_name in dset_names:\n",
    "                if dataset_name in self.channel_like:\n",
    "                    if self.context == -1:\n",
    "                        x_data.append(hf[dataset_name][:])\n",
    "                    else:\n",
    "                        x_data.append(hf[dataset_name][start_index : start_index + self.context])\n",
    "\n",
    "        if not x_data:\n",
    "            # Skip this data point if x_data is empty\n",
    "            return self.__getitem__((idx + 1) % self.total_len)\n",
    "\n",
    "        x_data = np.array(x_data)  # (C, T, F) assuming each channel returns (T, F)\n",
    "        x_data = torch.tensor(x_data, dtype=torch.float32)\n",
    "        y_data = torch.tensor(y_data, dtype=torch.float32)\n",
    "\n",
    "        min_length = min(x_data.shape[1], len(y_data))\n",
    "        x_data = x_data[:, :min_length, :]\n",
    "        y_data = y_data[:min_length]\n",
    "\n",
    "        return x_data, y_data, self.max_channels, self.max_seq_len, hdf5_path\n",
    "\n",
    "\n",
    "def sleep_event_finetune_full_collate_fn(batch):\n",
    "    x_data, y_data, max_channels_list, max_seq_len_list, hdf5_path_list = zip(*batch)\n",
    "\n",
    "    num_channels = max(max_channels_list)\n",
    "\n",
    "    max_seq_len_temp = max([item.size(1) for item in x_data])\n",
    "    # Determine the max sequence length for padding\n",
    "    if max_seq_len_list[0] is None:\n",
    "        max_seq_len = max_seq_len_temp\n",
    "    else:\n",
    "        max_seq_len = min(max_seq_len_temp, max_seq_len_list[0])\n",
    "\n",
    "    padded_x_data = []\n",
    "    padded_y_data = []\n",
    "    padded_mask = []\n",
    "\n",
    "    for x_item, y_item in zip(x_data, y_data):\n",
    "\n",
    "        # first non-zero index of y_data\n",
    "        #print(y_item.shape)\n",
    "\n",
    "\n",
    "        tgt_sleep_no_sleep = np.where(y_item > 0, 1, 0)\n",
    "        moving_avg_tgt_sleep_no_sleep = np.convolve(tgt_sleep_no_sleep, np.ones(1080)/1080, mode='valid')\n",
    "        try:\n",
    "            first_non_zero_index = np.where(moving_avg_tgt_sleep_no_sleep > 0.5)[0][0]\n",
    "        except IndexError:\n",
    "            first_non_zero_index = 0\n",
    "\n",
    "\n",
    "\n",
    "        #non_zero_indices = (y_item != 0).nonzero(as_tuple=True)[0]\n",
    "        #first_non_zero_index = non_zero_indices[0].item() - 20\n",
    "        if first_non_zero_index < 0:\n",
    "            first_non_zero_index = 0\n",
    "\n",
    "        #first_non_zero_index = 0\n",
    "\n",
    "        #print(f\"First non-zero index of y_data: {first_non_zero_index}\")\n",
    "        # Get the shape of x_item\n",
    "        c, s, e = x_item.size()\n",
    "        c = min(c, num_channels)\n",
    "        s = min(s, max_seq_len + first_non_zero_index)  # Ensure the sequence length doesn't exceed max_seq_len\n",
    "\n",
    "        # Create a padded tensor and a mask tensor for x_data\n",
    "        padded_x_item = torch.zeros((num_channels, max_seq_len, e))\n",
    "        mask = torch.ones((num_channels, max_seq_len))\n",
    "\n",
    "        # Copy the actual data to the padded tensor and set the mask for real data\n",
    "        #print(f\"Shape of x_item: {x_item[:c, first_non_zero_index:s, :e].shape}\")\n",
    "        padded_x_item[:c, :s-first_non_zero_index, :e] = x_item[:c, first_non_zero_index:s, :e]\n",
    "        mask[:c, :s-first_non_zero_index] = 0  # 0 for real data, 1 for padding\n",
    "\n",
    "        # Pad y_data with zeros to match max_seq_len\n",
    "        padded_y_item = torch.zeros(max_seq_len)\n",
    "        padded_y_item[:s-first_non_zero_index] = y_item[first_non_zero_index:s]\n",
    "\n",
    "        # Append padded items to lists\n",
    "        padded_x_data.append(padded_x_item)\n",
    "        padded_y_data.append(padded_y_item)\n",
    "        padded_mask.append(mask)\n",
    "\n",
    "    # Stack all tensors into a batch\n",
    "    x_data = torch.stack(padded_x_data)\n",
    "    y_data = torch.stack(padded_y_data)\n",
    "    padded_mask = torch.stack(padded_mask)\n",
    "\n",
    "    '''\n",
    "    for y_data_mini in y_data:\n",
    "        unique_labels = torch.unique(y_data_mini)\n",
    "        print(f\"Unique labels in batch: {unique_labels}\")\n",
    "    '''\n",
    "\n",
    "    return x_data, y_data, padded_mask, hdf5_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_paths = [\"demo_emb/demo_psg.hdf5\"]\n",
    "label_files = [\"demo_psg.csv\"]\n",
    "test_dataset = SleepEventClassificationDataset(sleep_staging_config, channel_groups, split=\"test\", hdf5_paths=hdf5_paths, label_files=label_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=1, collate_fn=sleep_event_finetune_full_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00,  1.33it/s]\n"
     ]
    }
   ],
   "source": [
    "# Validation loop at the end of each epoch\n",
    "model.eval()\n",
    "all_targets = []\n",
    "all_logits = []\n",
    "all_outputs = []\n",
    "all_masks = []\n",
    "all_paths = []\n",
    "\n",
    "count = 0\n",
    "with torch.no_grad():\n",
    "    for (x_data, y_data, padded_matrix, hdf5_path_list) in tqdm.tqdm(test_loader, desc=\"Evaluating\"):\n",
    "        x_data, y_data, padded_matrix, hdf5_path_list = x_data.to(device), y_data.to(device), padded_matrix.to(device), list(hdf5_path_list)\n",
    "        outputs, mask = sleep_staging_model(x_data, padded_matrix)\n",
    "        all_targets.append(y_data.cpu().numpy())\n",
    "        all_outputs.append(torch.softmax(outputs, dim=-1).cpu().numpy())\n",
    "        all_logits.append(outputs.cpu().numpy())\n",
    "        all_masks.append(mask.cpu().numpy())\n",
    "        all_paths.append(hdf5_path_list)\n",
    "\n",
    "\n",
    "save_path = \"demo_sleep_staging\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "targets_path = os.path.join(save_path, \"all_targets.pickle\")\n",
    "outputs_path = os.path.join(save_path, \"all_outputs.pickle\")\n",
    "logits_path = os.path.join(save_path, \"all_logits.pickle\")\n",
    "mask_path = os.path.join(save_path, \"all_masks.pickle\")\n",
    "file_paths = os.path.join(save_path, \"all_paths.pickle\")\n",
    "\n",
    "save_data(all_targets, targets_path)\n",
    "save_data(all_outputs, outputs_path)\n",
    "save_data(all_logits, logits_path)\n",
    "save_data(all_masks, mask_path)\n",
    "save_data(all_paths, file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 6960, 5), (1, 6960))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_outputs[0].shape, all_targets[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 1, 1)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_logits), len(all_outputs), len(all_targets), len(all_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 6960, 5), (1, 6960, 5), (1, 6960), (1, 6960))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_logits[0].shape, all_outputs[0].shape, all_targets[0].shape, all_masks[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_logits_flat = [logits.reshape(-1, logits.shape[-1]) for logits in all_logits]\n",
    "all_outputs_flat = [outputs.reshape(-1, outputs.shape[-1]) for outputs in all_outputs]\n",
    "all_targets_flat = [targets.reshape(-1) for targets in all_targets]\n",
    "all_masks_flat = [mask.reshape(-1) for mask in all_masks]\n",
    "\n",
    "# Convert lists of flattened arrays to single concatenated arrays if desired\n",
    "all_logits_flat = np.concatenate(all_logits_flat, axis=0)\n",
    "all_outputs_flat = np.concatenate(all_outputs_flat, axis=0)\n",
    "all_targets_flat = np.concatenate(all_targets_flat, axis=0)\n",
    "all_masks_flat = np.concatenate(all_masks_flat, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6960, 5), (6960, 5), (6960,), (6960,))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_logits_flat.shape, all_outputs_flat.shape, all_targets_flat.shape, all_masks_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_filter = all_masks_flat == 0\n",
    "\n",
    "# Apply the mask to each flattened array\n",
    "all_logits_filtered = all_logits_flat[mask_filter]\n",
    "all_outputs_filtered = all_outputs_flat[mask_filter]\n",
    "all_targets_filtered = all_targets_flat[mask_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0: 0.26814031715521386,\n",
       " 1.0: 0.028351753964440174,\n",
       " 2.0: 0.2715040845747237,\n",
       " 3.0: 0.15281114848630467,\n",
       " 4.0: 0.27919269581931766}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = Counter(all_targets_filtered)\n",
    "total = sum(counts.values())\n",
    "prevalence_dict = {cls: count / total for cls, count in counts.items()}\n",
    "prevalence_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = [\"Wake\", \"Stage 1\", \"Stage 2\", \"Stage 3\", \"REM\"]\n",
    "# class_labels = [\"No-Apnea\", \"Apnea\"]\n",
    "class_mapping = {label: idx for idx, label in enumerate(class_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score for Wake: 0.959\n",
      "F1 Score for Stage 1: 0.352\n",
      "F1 Score for Stage 2: 0.617\n",
      "F1 Score for Stage 3: 0.686\n",
      "F1 Score for REM: 0.964\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 1: Get predicted labels (argmax on probabilities)\n",
    "predicted_labels = np.argmax(all_outputs_filtered, axis=1)\n",
    "\n",
    "fontsize = 12\n",
    "\n",
    "# Step 2: Compute F1 score for each class\n",
    "f1_scores = f1_score(all_targets_filtered, predicted_labels, average=None, labels=range(len(class_labels)))\n",
    "for idx, label in enumerate(class_labels):\n",
    "    print(f\"F1 Score for {label}: {f1_scores[idx]:.3f}\")\n",
    "\n",
    "# Step 3: Create a confusion matrix and normalize it by row to get percentages\n",
    "conf_matrix = confusion_matrix(all_targets_filtered, predicted_labels, labels=range(len(class_labels)))\n",
    "conf_matrix_percent = conf_matrix / conf_matrix.sum(axis=1, keepdims=True) * 100\n",
    "\n",
    "# Plotting the confusion matrix with percentages\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(\n",
    "    conf_matrix_percent,\n",
    "    annot=True,\n",
    "    fmt=\".1f\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=class_labels,\n",
    "    yticklabels=class_labels,\n",
    "    annot_kws={\"size\": fontsize},  # Font size for numbers inside the heatmap\n",
    "    cbar_kws={\"shrink\": 1},  # Adjust colorbar size\n",
    ")\n",
    "\n",
    "# Customizing axis labels and ticks\n",
    "plt.xlabel(\"Predicted Labels\", fontsize=fontsize)\n",
    "plt.ylabel(\"True Labels\", fontsize=fontsize)\n",
    "plt.xticks(fontsize=12, ha=\"center\")  # Font size for x-axis tick labels with rotation\n",
    "plt.yticks(fontsize=12)  # Font size for y-axis tick labels\n",
    "\n",
    "# Adjust layout and save the figure\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3: Disease Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_model_path = \"../sleepfm/checkpoints/model_diagnosis\"\n",
    "config = load_data(os.path.join(disease_model_path, \"config.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"model_params\"][\"dropout\"] = 0.0\n",
    "model_params = config['model_params']\n",
    "model_class = getattr(sys.modules[__name__], config['model'])\n",
    "model = model_class(**model_params).to(device)\n",
    "model_name = type(model).__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized: DiagnosisFinetuneFullLSTMCOXPHWithDemo\n",
      "Trainable parameters: 0.91 million\n",
      "Number of layers: 15\n"
     ]
    }
   ],
   "source": [
    "model = nn.DataParallel(model)\n",
    "print(f\"Model initialized: {model_name}\")\n",
    "total_layers, total_params = count_parameters(model)\n",
    "print(f'Trainable parameters: {total_params / 1e6:.2f} million')\n",
    "print(f'Number of layers: {total_layers}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(disease_model_path, \"best.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(checkpoint_path)\n",
    "model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiagnosisFinetuneFullCOXPHWithDemoDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 config,\n",
    "                 channel_groups,\n",
    "                 hdf5_paths=None,\n",
    "                 demo_labels_path=None, \n",
    "                 split=\"train\"):\n",
    "\n",
    "        self.config = config\n",
    "        self.channel_groups = channel_groups\n",
    "        self.max_channels = self.config[\"max_channels\"]\n",
    "\n",
    "        # --- Load demographic features ---\n",
    "        if not demo_labels_path:\n",
    "            demo_labels_path = config[\"demo_labels_path\"]\n",
    "\n",
    "        demo_labels_df = pd.read_csv(demo_labels_path)\n",
    "        demo_labels_df = demo_labels_df.set_index(\"Study ID\")\n",
    "        study_ids = set(demo_labels_df.index)\n",
    "        print(study_ids)\n",
    "\n",
    "        # --- Resolve HDF5 paths (explicit precedence) ---\n",
    "        if hdf5_paths:\n",
    "            # Use provided paths directly\n",
    "            hdf5_paths = [f for f in hdf5_paths if os.path.exists(f)]\n",
    "        else:\n",
    "            # Load from split file\n",
    "            split_paths = load_data(config[\"split_path\"])[split]\n",
    "            hdf5_paths = [f for f in split_paths if os.path.exists(f)]\n",
    "\n",
    "        # Filter by available demo labels\n",
    "        hdf5_paths = [\n",
    "            f for f in hdf5_paths\n",
    "            if os.path.basename(f).split(\".\")[0] in study_ids\n",
    "        ]\n",
    "\n",
    "        # Optional truncation\n",
    "        if config.get(\"max_files\"):\n",
    "            hdf5_paths = hdf5_paths[:config[\"max_files\"]]\n",
    "\n",
    "        # --- Build labels dict (demo only) ---\n",
    "        labels_dict = {}\n",
    "        for study_id in tqdm.tqdm(study_ids, desc=\"Loading demo features\"):\n",
    "            labels_dict[study_id] = {\n",
    "                \"demo_feats\": list(demo_labels_df.loc[study_id].values)\n",
    "            }\n",
    "\n",
    "        # --- Build index map ---\n",
    "        self.index_map = [\n",
    "            (path, labels_dict[os.path.basename(path).split(\".\")[0]])\n",
    "            for path in hdf5_paths\n",
    "        ]\n",
    "\n",
    "        print(f\"Number of files in {split} set: {len(hdf5_paths)}\")\n",
    "        print(f\"Number of files to be processed in {split} set: {len(self.index_map)}\")\n",
    "\n",
    "        self.total_len = len(self.index_map)\n",
    "        self.max_seq_len = config[\"model_params\"][\"max_seq_length\"]\n",
    "\n",
    "        if self.total_len == 0:\n",
    "            raise ValueError(f\"No valid HDF5 files found for split='{split}'.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        hdf5_path, label_dict = self.index_map[idx]\n",
    "\n",
    "        demo_feats = label_dict[\"demo_feats\"]\n",
    "\n",
    "        x_data = []\n",
    "        with h5py.File(hdf5_path, \"r\") as hf:\n",
    "            dset_names = [\n",
    "                dset_name for dset_name in hf.keys()\n",
    "                if isinstance(hf[dset_name], h5py.Dataset)\n",
    "                and dset_name in self.config[\"modality_types\"]\n",
    "            ]\n",
    "\n",
    "            random.shuffle(dset_names)\n",
    "            for dataset_name in dset_names:\n",
    "                x_data.append(hf[dataset_name][:])\n",
    "\n",
    "        if not x_data:\n",
    "            # Skip empty sample\n",
    "            return self.__getitem__((idx + 1) % self.total_len)\n",
    "\n",
    "        # Convert to tensor\n",
    "        x_data = torch.tensor(np.array(x_data), dtype=torch.float32)\n",
    "        demo_feats = torch.tensor(demo_feats, dtype=torch.float32)\n",
    "\n",
    "        return x_data, demo_feats, self.max_channels, self.max_seq_len, hdf5_path\n",
    "\n",
    "\n",
    "def diagnosis_finetune_full_coxph_with_demo_collate_fn(batch):\n",
    "    x_data, demo_feats, max_channels_list, max_seq_len_list, hdf5_path_list = zip(*batch)\n",
    "\n",
    "    num_channels = max(max_channels_list)\n",
    "\n",
    "    if max_seq_len_list[0] == None:\n",
    "        max_seq_len = max([item.size(1) for item in x_data])\n",
    "    else:\n",
    "        max_seq_len = max_seq_len_list[0]\n",
    "\n",
    "    padded_x_data = []\n",
    "    padded_mask = []\n",
    "    for item in x_data:\n",
    "        c, s, e = item.size()\n",
    "        c = min(c, num_channels)\n",
    "        s = min(s, max_seq_len)  # Ensure the sequence length doesn't exceed max_seq_len\n",
    "\n",
    "        # Create a padded tensor and a mask tensor\n",
    "        padded_item = torch.zeros((num_channels, max_seq_len, e))\n",
    "        mask = torch.ones((num_channels, max_seq_len))\n",
    "\n",
    "        # Copy the actual data to the padded tensor and set the mask for real data\n",
    "        padded_item[:c, :s, :e] = item[:c, :s, :e]\n",
    "        mask[:c, :s] = 0  # 0 for real data, 1 for padding\n",
    "\n",
    "        padded_x_data.append(padded_item)\n",
    "        padded_mask.append(mask)\n",
    "\n",
    "    # Stack all tensors into a batch\n",
    "    x_data = torch.stack(padded_x_data)\n",
    "    demo_feats = torch.stack(demo_feats)\n",
    "    padded_mask = torch.stack(padded_mask)\n",
    "\n",
    "    return x_data, demo_feats, padded_mask, hdf5_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"demo_diagnosis\"\n",
    "os.makedirs(save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'demo_psg'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading demo features: 100%|██████████| 1/1 [00:00<00:00, 2563.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in test set: 1\n",
      "Number of files to be processed in test set: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "hdf5_paths = [\"demo_emb/demo_psg.hdf5\"]\n",
    "demo_labels_path = \"demo_age_gender.csv\"\n",
    "test_dataset = DiagnosisFinetuneFullCOXPHWithDemoDataset(config, channel_groups, split=\"test\", hdf5_paths=hdf5_paths, demo_labels_path=demo_labels_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=1, collate_fn=diagnosis_finetune_full_coxph_with_demo_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]/home/groups/jamesz/rthapa84/anaconda3/envs/sleepfm_clinical/lib/python3.10/site-packages/torch/nn/modules/transformer.py:562: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._transformer_encoder_layer_fwd(\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00,  1.20it/s]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "all_event_times = []\n",
    "all_is_event = []\n",
    "all_outputs = []\n",
    "all_paths = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for item in tqdm.tqdm(test_loader, desc=\"Evaluating\"):\n",
    "        x_data, demo_feats, padded_matrix, hdf5_path_list = item\n",
    "        x_data, demo_feats, padded_matrix, hdf5_path_list = x_data.to(device), demo_feats.to(device), padded_matrix.to(device), list(hdf5_path_list)\n",
    "        outputs = model(x_data, padded_matrix, demo_feats)\n",
    "    \n",
    "        logits = outputs.cpu().numpy()\n",
    "        all_outputs.append(logits)\n",
    "        all_paths.append(hdf5_path_list)\n",
    "\n",
    "all_outputs = np.concatenate(all_outputs, axis=0)\n",
    "all_paths = np.concatenate(all_paths)\n",
    "\n",
    "outputs_path = os.path.join(save_path, \"all_outputs.pickle\")\n",
    "file_paths = os.path.join(save_path, \"all_paths.pickle\")\n",
    "\n",
    "save_data(all_outputs, outputs_path)\n",
    "save_data(all_paths, file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1065)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, you get the model outputs, which you can then use to look for specific disease diagnosis. Nope that the shape of the output above is 1065, meaning, this model gives logprobs for 1065 conditions. We provide information about each disease index and its corresponding phecode here `sleepfm/configs/label_mapping.csv`. You can map it as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = pd.read_csv(\"../sleepfm/configs/label_mapping.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df[\"output\"] = all_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labels_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlabels_df\u001b[49m\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'labels_df' is not defined"
     ]
    }
   ],
   "source": [
    "labels_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sleepfm_clinical",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
