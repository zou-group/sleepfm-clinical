{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: All data in this repo is synthetically made including sleep stage annotations, demographics or diseases. The data is for demo purposes only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing Details\n",
    "\n",
    "\n",
    "Before running this notebook, please preprocess your PSG files using the scripts provided in `sleepfm/preprocessing`. Note that PSG recordings may contain different sets of channels across datasets. The predefined channel–modality mappings used in this project are specified in `sleepfm/configs/channel_groups.json`.\n",
    "\n",
    "Although we have attempted to make this mapping as comprehensive as possible, we strongly recommend reviewing the channels present in your specific PSG data. In consultation with domain experts, you should group any additional or dataset-specific channels into the appropriate modality categories and update `channel_groups.json` accordingly. This step is critical to ensure that all channels are correctly aligned with their intended modalities during preprocessing and downstream modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "import os\n",
    "import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../sleepfm\")\n",
    "from preprocessing.preprocessing import EDFToHDF5Converter\n",
    "from models.dataset import SetTransformerDataset, collate_fn\n",
    "from models.models import SetTransformer, SleepEventLSTMClassifier, DiagnosisFinetuneFullLSTMCOXPHWithDemo\n",
    "import h5py\n",
    "from utils import load_config, load_data, save_data, count_parameters\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 0: Preprocessing EDF files\n",
    "\n",
    "Note: This is just a demo notebook that preprocesses a single, specific file. run `sleepfm/preprocessing/preprocessing.sh` with appropriate folders to generate multiple preprocessed files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_save_path = \"demo_data\"\n",
    "os.makedirs(base_save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-05 23:31:16.528\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpreprocessing.preprocessing\u001b[0m:\u001b[36mread_edf\u001b[0m:\u001b[36m146\u001b[0m - \u001b[1mreading edf\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from /home/groups/jamesz/rthapa84/repos/sleepfm-clinical/notebooks/demo_data/demo_psg.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 2234623  =      0.000 ...  8728.996 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-05 23:31:16.698\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpreprocessing.preprocessing\u001b[0m:\u001b[36mresample_signals\u001b[0m:\u001b[36m184\u001b[0m - \u001b[1mresampling signals\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering signal\n",
      "Filtering signal\n",
      "Filtering signal\n",
      "Filtering signal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-05 23:31:17.007\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpreprocessing.preprocessing\u001b[0m:\u001b[36msave_to_hdf5\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1msaving hdf5\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "root_dir = \"/edf_root\"      # dummy root not used for a single file conversion\n",
    "target_dir = \"/note\"    # dummy target not used for a single file conversion\n",
    "\n",
    "edf_path = \"demo_data/demo_psg.edf\"\n",
    "hdf5_path = os.path.join(base_save_path, \"demo_psg.hdf5\")\n",
    "\n",
    "converter = EDFToHDF5Converter(\n",
    "    root_dir=root_dir,\n",
    "    target_dir=target_dir,\n",
    "    resample_rate=128\n",
    ")\n",
    "\n",
    "# run for single file conversion\n",
    "converter.convert(edf_path, hdf5_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1: Generating embeddings from SleepFM pretrained model\n",
    "\n",
    "Here we show generating embedding for 1 demno PSG. To see full script, please check `sleepfm/pipeline/generate_embeddings.py`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../sleepfm/checkpoints/model_base\"\n",
    "channel_groups_path = \"../sleepfm/configs/channel_groups.json\"\n",
    "config_path = os.path.join(model_path, \"config.json\")\n",
    "\n",
    "config = load_config(config_path)\n",
    "channel_groups = load_data(channel_groups_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "modality_types = config[\"modality_types\"]\n",
    "in_channels = config[\"in_channels\"]\n",
    "patch_size = config[\"patch_size\"]\n",
    "embed_dim = config[\"embed_dim\"]\n",
    "num_heads = config[\"num_heads\"]\n",
    "num_layers = config[\"num_layers\"]\n",
    "pooling_head = config[\"pooling_head\"]\n",
    "dropout = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 4.44 million\n",
      "Number of layers: 93\n"
     ]
    }
   ],
   "source": [
    "model_class = getattr(sys.modules[__name__], config['model'])\n",
    "model = model_class(in_channels, patch_size, embed_dim, num_heads, num_layers, pooling_head=pooling_head, dropout=dropout)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "if device.type == \"cuda\":\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "model.to(device)\n",
    "total_layers, total_params = count_parameters(model)\n",
    "print(f'Trainable parameters: {total_params / 1e6:.2f} million')\n",
    "print(f'Number of layers: {total_layers}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): SetTransformer(\n",
       "    (patch_embedding): Tokenizer(\n",
       "      (tokenizer): Sequential(\n",
       "        (0): Conv1d(1, 4, kernel_size=(5,), stride=(2,), padding=(2,))\n",
       "        (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ELU(alpha=1.0)\n",
       "        (3): LayerNorm((4, 320), eps=1e-05, elementwise_affine=True)\n",
       "        (4): Conv1d(4, 8, kernel_size=(5,), stride=(2,), padding=(2,))\n",
       "        (5): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (6): ELU(alpha=1.0)\n",
       "        (7): LayerNorm((8, 160), eps=1e-05, elementwise_affine=True)\n",
       "        (8): Conv1d(8, 16, kernel_size=(5,), stride=(2,), padding=(2,))\n",
       "        (9): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (10): ELU(alpha=1.0)\n",
       "        (11): LayerNorm((16, 80), eps=1e-05, elementwise_affine=True)\n",
       "        (12): Conv1d(16, 32, kernel_size=(5,), stride=(2,), padding=(2,))\n",
       "        (13): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (14): ELU(alpha=1.0)\n",
       "        (15): LayerNorm((32, 40), eps=1e-05, elementwise_affine=True)\n",
       "        (16): Conv1d(32, 64, kernel_size=(5,), stride=(2,), padding=(2,))\n",
       "        (17): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (18): ELU(alpha=1.0)\n",
       "        (19): LayerNorm((64, 20), eps=1e-05, elementwise_affine=True)\n",
       "        (20): Conv1d(64, 128, kernel_size=(5,), stride=(2,), padding=(2,))\n",
       "        (21): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (22): ELU(alpha=1.0)\n",
       "        (23): LayerNorm((128, 10), eps=1e-05, elementwise_affine=True)\n",
       "        (24): AdaptiveAvgPool1d(output_size=1)\n",
       "        (25): Flatten(start_dim=1, end_dim=-1)\n",
       "        (26): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (spatial_pooling): AttentionPooling(\n",
       "      (transformer_layer): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (positional_encoding): PositionalEncoding()\n",
       "    (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer_encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (temporal_pooling): AttentionPooling(\n",
       "      (transformer_layer): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(os.path.join(model_path, \"best.pt\"))\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'seed': 42,\n",
       " 'model': 'SetTransformer',\n",
       " 'in_channels': 1,\n",
       " 'batch_size': 128,\n",
       " 'epochs': 1,\n",
       " 'lr': 0.001,\n",
       " 'lr_step_period': 2,\n",
       " 'gamma': 0.1,\n",
       " 'temperature': 0.0,\n",
       " 'momentum': 0.9,\n",
       " 'num_workers': 16,\n",
       " 'embed_dim': 128,\n",
       " 'num_heads': 8,\n",
       " 'num_layers': 6,\n",
       " 'pooling_head': 8,\n",
       " 'dropout': 0.3,\n",
       " 'split_path': 'path_to_/dataset_split.json',\n",
       " 'save_path': 'path_to_/models',\n",
       " 'weight_decay': 0.0,\n",
       " 'mode': 'leave_one_out',\n",
       " 'save_iter': 5000,\n",
       " 'eval_iter': 5000,\n",
       " 'log_interval': 100,\n",
       " 'use_wandb': True,\n",
       " 'BAS_CHANNELS': 10,\n",
       " 'RESP_CHANNELS': 7,\n",
       " 'EKG_CHANNELS': 2,\n",
       " 'EMG_CHANNELS': 4,\n",
       " 'max_files': None,\n",
       " 'val_size': 100,\n",
       " 'sampling_duration': 5,\n",
       " 'sampling_freq': 128,\n",
       " 'patch_size': 640,\n",
       " 'modality_types': ['BAS', 'RESP', 'EKG', 'EMG']}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing files: 100%|██████████| 1/1 [00:00<00:00, 110.49it/s]\n"
     ]
    }
   ],
   "source": [
    "hdf5_paths = [os.path.join(base_save_path, \"demo_psg.hdf5\")]\n",
    "dataset = SetTransformerDataset(config, channel_groups, hdf5_paths=hdf5_paths, split=\"test\")\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, \n",
    "                                            batch_size=16, \n",
    "                                            num_workers=1, \n",
    "                                            shuffle=False, \n",
    "                                            collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = os.path.join(base_save_path, \"demo_emb\")\n",
    "output_5min_agg = os.path.join(base_save_path, \"demo_5min_agg_emb\")\n",
    "os.makedirs(output, exist_ok=True)\n",
    "os.makedirs(output_5min_agg, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:08<00:00,  4.38s/it]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    with tqdm.tqdm(total=len(dataloader)) as pbar:\n",
    "        for batch in dataloader:\n",
    "            batch_data, mask_list, file_paths, dset_names_list, chunk_starts = batch\n",
    "            (bas, resp, ekg, emg) = batch_data\n",
    "            (mask_bas, mask_resp, mask_ekg, mask_emg) = mask_list\n",
    "\n",
    "            bas = bas.to(device, dtype=torch.float)\n",
    "            resp = resp.to(device, dtype=torch.float)\n",
    "            ekg = ekg.to(device, dtype=torch.float)\n",
    "            emg = emg.to(device, dtype=torch.float)\n",
    "\n",
    "            mask_bas = mask_bas.to(device, dtype=torch.bool)\n",
    "            mask_resp = mask_resp.to(device, dtype=torch.bool)\n",
    "            mask_ekg = mask_ekg.to(device, dtype=torch.bool)\n",
    "            mask_emg = mask_emg.to(device, dtype=torch.bool)\n",
    "\n",
    "            embeddings = [\n",
    "                model(bas, mask_bas),\n",
    "                model(resp, mask_resp),\n",
    "                model(ekg, mask_ekg),\n",
    "                model(emg, mask_emg),\n",
    "            ]\n",
    "\n",
    "            # Model gives two kinds of embeddings. Granular 5 second-level embeddings and aggregated 5 minute-level embeddings. We save both of them below. \n",
    "\n",
    "            embeddings_new = [e[0].unsqueeze(1) for e in embeddings]\n",
    "\n",
    "            for i in range(len(file_paths)):\n",
    "                file_path = file_paths[i]\n",
    "                chunk_start = chunk_starts[i]\n",
    "                subject_id = os.path.basename(file_path).split('.')[0]\n",
    "                output_path = os.path.join(output_5min_agg, f\"{subject_id}.hdf5\")\n",
    "\n",
    "                with h5py.File(output_path, 'a') as hdf5_file:\n",
    "                    for modality_idx, modality_type in enumerate(config[\"modality_types\"]):\n",
    "                        if modality_type in hdf5_file:\n",
    "                            dset = hdf5_file[modality_type]\n",
    "                            chunk_start_correct = chunk_start // (embed_dim * 5 * 60)\n",
    "                            chunk_end = chunk_start_correct + embeddings_new[modality_idx][i].shape[0]\n",
    "                            if dset.shape[0] < chunk_end:\n",
    "                                dset.resize((chunk_end,) + embeddings_new[modality_idx][i].shape[1:])\n",
    "                            dset[chunk_start_correct:chunk_end] = embeddings_new[modality_idx][i].cpu().numpy()\n",
    "                        else:\n",
    "                            hdf5_file.create_dataset(modality_type, data=embeddings_new[modality_idx][i].cpu().numpy(), chunks=(embed_dim,) + embeddings_new[modality_idx][i].shape[1:], maxshape=(None,) + embeddings_new[modality_idx][i].shape[1:])\n",
    "\n",
    "            embeddings_new = [e[1] for e in embeddings]\n",
    "\n",
    "            for i in range(len(file_paths)):\n",
    "                file_path = file_paths[i]\n",
    "                chunk_start = chunk_starts[i]\n",
    "                subject_id = os.path.basename(file_path).split('.')[0]\n",
    "                output_path = os.path.join(output, f\"{subject_id}.hdf5\")\n",
    "\n",
    "                with h5py.File(output_path, 'a') as hdf5_file:\n",
    "                    for modality_idx, modality_type in enumerate(config[\"modality_types\"]):\n",
    "                        if modality_type in hdf5_file:\n",
    "                            dset = hdf5_file[modality_type]\n",
    "                            chunk_start_correct = chunk_start // (embed_dim * 5)\n",
    "                            chunk_end = chunk_start_correct + embeddings_new[modality_idx][i].shape[0]\n",
    "                            if dset.shape[0] < chunk_end:\n",
    "                                dset.resize((chunk_end,) + embeddings_new[modality_idx][i].shape[1:])\n",
    "                            dset[chunk_start_correct:chunk_end] = embeddings_new[modality_idx][i].cpu().numpy()\n",
    "                        else:\n",
    "                            hdf5_file.create_dataset(modality_type, data=embeddings_new[modality_idx][i].cpu().numpy(), chunks=(embed_dim,) + embeddings_new[modality_idx][i].shape[1:], maxshape=(None,) + embeddings_new[modality_idx][i].shape[1:])\n",
    "            pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2: Sleep Staging\n",
    "\n",
    "Note that below, we are using our finetuned sleep staging model. It is always a good idea to finetune our model on your specific data, even if you only have a handful of sample, so that the model can adapt to your specific data distribution. Script to finetune your sleep staging model head is given in `sleepfm/pipeline/finetune_sleep_staging.py`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_staging_model_path = \"../sleepfm/checkpoints/model_sleep_staging\"\n",
    "sleep_staging_config = load_data(os.path.join(sleep_staging_model_path, \"config.json\"))\n",
    "\n",
    "sleep_staging_model_params = sleep_staging_config['model_params']\n",
    "sleep_staging_model_class = getattr(sys.modules[__name__], sleep_staging_config['model'])\n",
    "\n",
    "sleep_staging_model = sleep_staging_model_class(**sleep_staging_model_params).to(device)\n",
    "sleep_staging_model_name = type(sleep_staging_model).__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 GPUs\n"
     ]
    }
   ],
   "source": [
    "sleep_staging_model = nn.DataParallel(sleep_staging_model)\n",
    "print(f\"Using {torch.cuda.device_count()} GPUs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized: SleepEventLSTMClassifier\n",
      "Trainable parameters: 1.19 million\n",
      "Number of layers: 20\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model initialized: {sleep_staging_model_name}\")\n",
    "total_layers, total_params = count_parameters(sleep_staging_model)\n",
    "print(f'Trainable parameters: {total_params / 1e6:.2f} million')\n",
    "print(f'Number of layers: {total_layers}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sleep_staging_checkpoint_path = os.path.join(sleep_staging_model_path, \"best.pth\")\n",
    "sleep_staging_checkpoint = torch.load(sleep_staging_checkpoint_path)\n",
    "sleep_staging_model.load_state_dict(sleep_staging_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some helper functions for loading data for sleep staging. You can find similar functions within `sleepfm/models/dataset.py`. You may need to modify it slightly based on your usecase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SleepEventClassificationDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config,\n",
    "        channel_groups,\n",
    "        hdf5_paths,\n",
    "        label_files,\n",
    "        split=\"train\",\n",
    "    ):\n",
    "        self.config = config\n",
    "        self.max_channels = self.config[\"max_channels\"]\n",
    "        self.context = int(self.config[\"context\"])\n",
    "        self.channel_like = self.config[\"channel_like\"]\n",
    "\n",
    "        self.max_seq_len = config[\"model_params\"][\"max_seq_length\"]\n",
    "\n",
    "        # --- Build label lookup: {study_id: label_csv_path} ---\n",
    "        # study_id = filename without extension, e.g. \"SSC_12345\"\n",
    "        labels_dict = {\n",
    "            os.path.basename(p).rsplit(\".\", 1)[0]: p\n",
    "            for p in label_files\n",
    "            if os.path.exists(p)\n",
    "        }\n",
    "\n",
    "        # --- Filter to HDF5s that exist and have a matching label file ---\n",
    "        hdf5_paths = [p for p in hdf5_paths if os.path.exists(p)]\n",
    "        hdf5_paths = [\n",
    "            p for p in hdf5_paths\n",
    "            if os.path.basename(p).rsplit(\".\", 1)[0] in labels_dict\n",
    "        ]\n",
    "\n",
    "        if config.get(\"max_files\"):\n",
    "            hdf5_paths = hdf5_paths[: config[\"max_files\"]]\n",
    "\n",
    "        self.hdf5_paths = hdf5_paths\n",
    "        self.labels_dict = labels_dict\n",
    "\n",
    "        # --- Build index map ---\n",
    "        # Each item is (hdf5_path, label_path, start_index)\n",
    "        if self.context == -1:\n",
    "            self.index_map = [\n",
    "                (p, labels_dict[os.path.basename(p).rsplit(\".\", 1)[0]], -1)\n",
    "                for p in self.hdf5_paths\n",
    "            ]\n",
    "        else:\n",
    "            self.index_map = []\n",
    "            loop = tqdm(self.hdf5_paths, total=len(self.hdf5_paths), desc=f\"Indexing {split} data\")\n",
    "            for hdf5_file_path in loop:\n",
    "                file_prefix = os.path.basename(hdf5_file_path).rsplit(\".\", 1)[0]\n",
    "                label_path = labels_dict[file_prefix]\n",
    "\n",
    "                with h5py.File(hdf5_file_path, \"r\") as hf:\n",
    "                    dset_names = list(hf.keys())\n",
    "                    if len(dset_names) == 0:\n",
    "                        continue\n",
    "\n",
    "                    # Use first dataset to define length (same as your original behavior)\n",
    "                    first_name = dset_names[0]\n",
    "                    dataset_length = hf[first_name].shape[0]\n",
    "\n",
    "                for i in range(0, dataset_length, self.context):\n",
    "                    self.index_map.append((hdf5_file_path, label_path, i))\n",
    "\n",
    "        # If you have logger, keep; otherwise you can remove these.\n",
    "        # logger.info(f\"Number of files in {split} set: {len(self.hdf5_paths)}\")\n",
    "        # logger.info(f\"Number of files to be processed in {split} set: {len(self.index_map)}\")\n",
    "\n",
    "        self.total_len = len(self.index_map)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_len\n",
    "\n",
    "    def get_index_map(self):\n",
    "        return self.index_map\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        hdf5_path, label_path, start_index = self.index_map[idx]\n",
    "\n",
    "        labels_df = pd.read_csv(label_path)\n",
    "        labels_df[\"StageNumber\"] = labels_df[\"StageNumber\"].replace(-1, 0)\n",
    "\n",
    "        y_data = labels_df[\"StageNumber\"].to_numpy()\n",
    "        if self.context != -1:\n",
    "            y_data = y_data[start_index : start_index + self.context]\n",
    "\n",
    "        x_data = []\n",
    "        with h5py.File(hdf5_path, \"r\") as hf:\n",
    "            dset_names = list(hf.keys())\n",
    "\n",
    "            for dataset_name in dset_names:\n",
    "                if dataset_name in self.channel_like:\n",
    "                    if self.context == -1:\n",
    "                        x_data.append(hf[dataset_name][:])\n",
    "                    else:\n",
    "                        x_data.append(hf[dataset_name][start_index : start_index + self.context])\n",
    "\n",
    "        if not x_data:\n",
    "            # Skip this data point if x_data is empty\n",
    "            return self.__getitem__((idx + 1) % self.total_len)\n",
    "\n",
    "        x_data = np.array(x_data)  # (C, T, F) assuming each channel returns (T, F)\n",
    "        x_data = torch.tensor(x_data, dtype=torch.float32)\n",
    "        y_data = torch.tensor(y_data, dtype=torch.float32)\n",
    "\n",
    "        min_length = min(x_data.shape[1], len(y_data))\n",
    "        x_data = x_data[:, :min_length, :]\n",
    "        y_data = y_data[:min_length]\n",
    "\n",
    "        return x_data, y_data, self.max_channels, self.max_seq_len, hdf5_path\n",
    "\n",
    "\n",
    "def sleep_event_finetune_full_collate_fn(batch):\n",
    "    x_data, y_data, max_channels_list, max_seq_len_list, hdf5_path_list = zip(*batch)\n",
    "\n",
    "    num_channels = max(max_channels_list)\n",
    "\n",
    "    max_seq_len_temp = max([item.size(1) for item in x_data])\n",
    "    # Determine the max sequence length for padding\n",
    "    if max_seq_len_list[0] is None:\n",
    "        max_seq_len = max_seq_len_temp\n",
    "    else:\n",
    "        max_seq_len = min(max_seq_len_temp, max_seq_len_list[0])\n",
    "\n",
    "    padded_x_data = []\n",
    "    padded_y_data = []\n",
    "    padded_mask = []\n",
    "\n",
    "    for x_item, y_item in zip(x_data, y_data):\n",
    "\n",
    "        # first non-zero index of y_data\n",
    "        #print(y_item.shape)\n",
    "\n",
    "\n",
    "        tgt_sleep_no_sleep = np.where(y_item > 0, 1, 0)\n",
    "        moving_avg_tgt_sleep_no_sleep = np.convolve(tgt_sleep_no_sleep, np.ones(1080)/1080, mode='valid')\n",
    "        try:\n",
    "            first_non_zero_index = np.where(moving_avg_tgt_sleep_no_sleep > 0.5)[0][0]\n",
    "        except IndexError:\n",
    "            first_non_zero_index = 0\n",
    "\n",
    "\n",
    "\n",
    "        #non_zero_indices = (y_item != 0).nonzero(as_tuple=True)[0]\n",
    "        #first_non_zero_index = non_zero_indices[0].item() - 20\n",
    "        if first_non_zero_index < 0:\n",
    "            first_non_zero_index = 0\n",
    "\n",
    "        #first_non_zero_index = 0\n",
    "\n",
    "        #print(f\"First non-zero index of y_data: {first_non_zero_index}\")\n",
    "        # Get the shape of x_item\n",
    "        c, s, e = x_item.size()\n",
    "        c = min(c, num_channels)\n",
    "        s = min(s, max_seq_len + first_non_zero_index)  # Ensure the sequence length doesn't exceed max_seq_len\n",
    "\n",
    "        # Create a padded tensor and a mask tensor for x_data\n",
    "        padded_x_item = torch.zeros((num_channels, max_seq_len, e))\n",
    "        mask = torch.ones((num_channels, max_seq_len))\n",
    "\n",
    "        # Copy the actual data to the padded tensor and set the mask for real data\n",
    "        #print(f\"Shape of x_item: {x_item[:c, first_non_zero_index:s, :e].shape}\")\n",
    "        padded_x_item[:c, :s-first_non_zero_index, :e] = x_item[:c, first_non_zero_index:s, :e]\n",
    "        mask[:c, :s-first_non_zero_index] = 0  # 0 for real data, 1 for padding\n",
    "\n",
    "        # Pad y_data with zeros to match max_seq_len\n",
    "        padded_y_item = torch.zeros(max_seq_len)\n",
    "        padded_y_item[:s-first_non_zero_index] = y_item[first_non_zero_index:s]\n",
    "\n",
    "        # Append padded items to lists\n",
    "        padded_x_data.append(padded_x_item)\n",
    "        padded_y_data.append(padded_y_item)\n",
    "        padded_mask.append(mask)\n",
    "\n",
    "    # Stack all tensors into a batch\n",
    "    x_data = torch.stack(padded_x_data)\n",
    "    y_data = torch.stack(padded_y_data)\n",
    "    padded_mask = torch.stack(padded_mask)\n",
    "\n",
    "    '''\n",
    "    for y_data_mini in y_data:\n",
    "        unique_labels = torch.unique(y_data_mini)\n",
    "        print(f\"Unique labels in batch: {unique_labels}\")\n",
    "    '''\n",
    "\n",
    "    return x_data, y_data, padded_mask, hdf5_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_paths = [os.path.join(base_save_path, \"demo_emb/demo_psg.hdf5\")]\n",
    "label_files = [os.path.join(base_save_path, \"demo_psg.csv\")]\n",
    "test_dataset = SleepEventClassificationDataset(sleep_staging_config, channel_groups, split=\"test\", hdf5_paths=hdf5_paths, label_files=label_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=1, collate_fn=sleep_event_finetune_full_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1/1 [00:01<00:00,  1.60s/it]\n"
     ]
    }
   ],
   "source": [
    "# Validation loop at the end of each epoch\n",
    "model.eval()\n",
    "all_targets = []\n",
    "all_logits = []\n",
    "all_outputs = []\n",
    "all_masks = []\n",
    "all_paths = []\n",
    "\n",
    "count = 0\n",
    "with torch.no_grad():\n",
    "    for (x_data, y_data, padded_matrix, hdf5_path_list) in tqdm.tqdm(test_loader, desc=\"Evaluating\"):\n",
    "        x_data, y_data, padded_matrix, hdf5_path_list = x_data.to(device), y_data.to(device), padded_matrix.to(device), list(hdf5_path_list)\n",
    "        outputs, mask = sleep_staging_model(x_data, padded_matrix)\n",
    "        all_targets.append(y_data.cpu().numpy())\n",
    "        all_outputs.append(torch.softmax(outputs, dim=-1).cpu().numpy())\n",
    "        all_logits.append(outputs.cpu().numpy())\n",
    "        all_masks.append(mask.cpu().numpy())\n",
    "        all_paths.append(hdf5_path_list)\n",
    "\n",
    "\n",
    "save_path = os.path.join(base_save_path, \"demo_sleep_staging\")\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "targets_path = os.path.join(save_path, \"all_targets.pickle\")\n",
    "outputs_path = os.path.join(save_path, \"all_outputs.pickle\")\n",
    "logits_path = os.path.join(save_path, \"all_logits.pickle\")\n",
    "mask_path = os.path.join(save_path, \"all_masks.pickle\")\n",
    "file_paths = os.path.join(save_path, \"all_paths.pickle\")\n",
    "\n",
    "save_data(all_targets, targets_path)\n",
    "save_data(all_outputs, outputs_path)\n",
    "save_data(all_logits, logits_path)\n",
    "save_data(all_masks, mask_path)\n",
    "save_data(all_paths, file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 1740, 5), (1, 1740))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_outputs[0].shape, all_targets[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 1, 1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_logits), len(all_outputs), len(all_targets), len(all_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 1740, 5), (1, 1740, 5), (1, 1740), (1, 1740))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_logits[0].shape, all_outputs[0].shape, all_targets[0].shape, all_masks[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_logits_flat = [logits.reshape(-1, logits.shape[-1]) for logits in all_logits]\n",
    "all_outputs_flat = [outputs.reshape(-1, outputs.shape[-1]) for outputs in all_outputs]\n",
    "all_targets_flat = [targets.reshape(-1) for targets in all_targets]\n",
    "all_masks_flat = [mask.reshape(-1) for mask in all_masks]\n",
    "\n",
    "# Convert lists of flattened arrays to single concatenated arrays if desired\n",
    "all_logits_flat = np.concatenate(all_logits_flat, axis=0)\n",
    "all_outputs_flat = np.concatenate(all_outputs_flat, axis=0)\n",
    "all_targets_flat = np.concatenate(all_targets_flat, axis=0)\n",
    "all_masks_flat = np.concatenate(all_masks_flat, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1740, 5), (1740, 5), (1740,), (1740,))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_logits_flat.shape, all_outputs_flat.shape, all_targets_flat.shape, all_masks_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_filter = all_masks_flat == 0\n",
    "\n",
    "# Apply the mask to each flattened array\n",
    "all_logits_filtered = all_logits_flat[mask_filter]\n",
    "all_outputs_filtered = all_outputs_flat[mask_filter]\n",
    "all_targets_filtered = all_targets_flat[mask_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0: 0.367816091954023,\n",
       " 1.0: 0.1511494252873563,\n",
       " 4.0: 0.16839080459770114,\n",
       " 3.0: 0.15402298850574714,\n",
       " 2.0: 0.15862068965517243}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = Counter(all_targets_filtered)\n",
    "total = sum(counts.values())\n",
    "prevalence_dict = {cls: count / total for cls, count in counts.items()}\n",
    "prevalence_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = [\"Wake\", \"Stage 1\", \"Stage 2\", \"Stage 3\", \"REM\"]\n",
    "# class_labels = [\"No-Apnea\", \"Apnea\"]\n",
    "class_mapping = {label: idx for idx, label in enumerate(class_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score for Wake: 0.538\n",
      "F1 Score for Stage 1: 0.000\n",
      "F1 Score for Stage 2: 0.000\n",
      "F1 Score for Stage 3: 0.000\n",
      "F1 Score for REM: 0.000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 1: Get predicted labels (argmax on probabilities)\n",
    "predicted_labels = np.argmax(all_outputs_filtered, axis=1)\n",
    "\n",
    "fontsize = 12\n",
    "\n",
    "# Step 2: Compute F1 score for each class\n",
    "f1_scores = f1_score(all_targets_filtered, predicted_labels, average=None, labels=range(len(class_labels)))\n",
    "for idx, label in enumerate(class_labels):\n",
    "    print(f\"F1 Score for {label}: {f1_scores[idx]:.3f}\")\n",
    "\n",
    "# Step 3: Create a confusion matrix and normalize it by row to get percentages\n",
    "conf_matrix = confusion_matrix(all_targets_filtered, predicted_labels, labels=range(len(class_labels)))\n",
    "conf_matrix_percent = conf_matrix / conf_matrix.sum(axis=1, keepdims=True) * 100\n",
    "\n",
    "# Plotting the confusion matrix with percentages\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(\n",
    "    conf_matrix_percent,\n",
    "    annot=True,\n",
    "    fmt=\".1f\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=class_labels,\n",
    "    yticklabels=class_labels,\n",
    "    annot_kws={\"size\": fontsize},  # Font size for numbers inside the heatmap\n",
    "    cbar_kws={\"shrink\": 1},  # Adjust colorbar size\n",
    ")\n",
    "\n",
    "# Customizing axis labels and ticks\n",
    "plt.xlabel(\"Predicted Labels\", fontsize=fontsize)\n",
    "plt.ylabel(\"True Labels\", fontsize=fontsize)\n",
    "plt.xticks(fontsize=12, ha=\"center\")  # Font size for x-axis tick labels with rotation\n",
    "plt.yticks(fontsize=12)  # Font size for y-axis tick labels\n",
    "\n",
    "# Adjust layout and save the figure\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3: Disease Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_model_path = \"../sleepfm/checkpoints/model_diagnosis\"\n",
    "config = load_data(os.path.join(disease_model_path, \"config.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"model_params\"][\"dropout\"] = 0.0\n",
    "model_params = config['model_params']\n",
    "model_class = getattr(sys.modules[__name__], config['model'])\n",
    "model = model_class(**model_params).to(device)\n",
    "model_name = type(model).__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized: DiagnosisFinetuneFullLSTMCOXPHWithDemo\n",
      "Trainable parameters: 0.91 million\n",
      "Number of layers: 15\n"
     ]
    }
   ],
   "source": [
    "model = nn.DataParallel(model)\n",
    "print(f\"Model initialized: {model_name}\")\n",
    "total_layers, total_params = count_parameters(model)\n",
    "print(f'Trainable parameters: {total_params / 1e6:.2f} million')\n",
    "print(f'Number of layers: {total_layers}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(disease_model_path, \"best.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(checkpoint_path)\n",
    "model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiagnosisFinetuneFullCOXPHWithDemoDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 config,\n",
    "                 channel_groups,\n",
    "                 hdf5_paths=None,\n",
    "                 demo_labels_path=None,\n",
    "                 split=\"train\"):\n",
    "\n",
    "        self.config = config\n",
    "        self.channel_groups = channel_groups\n",
    "        self.max_channels = self.config[\"max_channels\"]\n",
    "\n",
    "        # --- Load demographic features ---\n",
    "        if not demo_labels_path:\n",
    "            demo_labels_path = config[\"demo_labels_path\"]\n",
    "\n",
    "        demo_labels_df = pd.read_csv(demo_labels_path)\n",
    "        demo_labels_df = demo_labels_df.set_index(\"Study ID\")\n",
    "        study_ids = set(demo_labels_df.index)\n",
    "\n",
    "        is_event_df = pd.read_csv(os.path.join(self.config[\"labels_path\"], \"is_event.csv\"))\n",
    "        event_time_df = pd.read_csv(os.path.join(self.config[\"labels_path\"], \"time_to_event.csv\"))\n",
    "\n",
    "        is_event_df = is_event_df.set_index('Study ID')\n",
    "        event_time_df = event_time_df.set_index('Study ID')\n",
    "\n",
    "        # --- Resolve HDF5 paths (explicit precedence) ---\n",
    "        if hdf5_paths:\n",
    "            # Use provided paths directly\n",
    "            hdf5_paths = [f for f in hdf5_paths if os.path.exists(f)]\n",
    "        else:\n",
    "            # Load from split file\n",
    "            split_paths = load_data(config[\"split_path\"])[split]\n",
    "            hdf5_paths = [f for f in split_paths if os.path.exists(f)]\n",
    "\n",
    "        # Filter by available demo labels\n",
    "        hdf5_paths = [\n",
    "            f for f in hdf5_paths\n",
    "            if os.path.basename(f).split(\".\")[0] in study_ids\n",
    "        ]\n",
    "\n",
    "        # Optional truncation\n",
    "        if config.get(\"max_files\"):\n",
    "            hdf5_paths = hdf5_paths[:config[\"max_files\"]]\n",
    "\n",
    "        labels_dict = {}\n",
    "        # Loop over each study_id\n",
    "        for study_id in tqdm.tqdm(study_ids):\n",
    "            # Extract the row as a whole for both dataframes (faster than iterating over columns)\n",
    "            is_event_row = list(is_event_df.loc[study_id].values)\n",
    "            event_time_row = list(event_time_df.loc[study_id].values)\n",
    "            demo_feats = list(demo_labels_df.loc[study_id].values)\n",
    "\n",
    "            # values = [[event_time, is_event] for is_event, event_time in zip(is_event_row, event_time_row)]\n",
    "            labels_dict[study_id] = {\n",
    "                \"is_event\": is_event_row,\n",
    "                \"event_time\": event_time_row, \n",
    "                \"demo_feats\": demo_feats\n",
    "            }\n",
    "\n",
    "        # --- Build index map ---\n",
    "        self.index_map = [\n",
    "            (path, labels_dict[os.path.basename(path).split(\".\")[0]])\n",
    "            for path in hdf5_paths\n",
    "        ]\n",
    "\n",
    "        print(f\"Number of files in {split} set: {len(hdf5_paths)}\")\n",
    "        print(f\"Number of files to be processed in {split} set: {len(self.index_map)}\")\n",
    "\n",
    "        self.total_len = len(self.index_map)\n",
    "        self.max_seq_len = config[\"model_params\"][\"max_seq_length\"]\n",
    "\n",
    "        if self.total_len == 0:\n",
    "            raise ValueError(f\"No valid HDF5 files found for split='{split}'.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        hdf5_path, tte_event = self.index_map[idx]\n",
    "\n",
    "        event_time = tte_event[\"event_time\"]\n",
    "        is_event = tte_event[\"is_event\"]\n",
    "        demo_feats = tte_event[\"demo_feats\"]\n",
    "\n",
    "        x_data = []\n",
    "        with h5py.File(hdf5_path, 'r') as hf:\n",
    "            dset_names = []\n",
    "            for dset_name in hf.keys():\n",
    "                if isinstance(hf[dset_name], h5py.Dataset) and dset_name in self.config[\"modality_types\"]:\n",
    "                    dset_names.append(dset_name)\n",
    "            \n",
    "            random.shuffle(dset_names)\n",
    "            for dataset_name in dset_names:\n",
    "                x_data.append(hf[dataset_name][:])\n",
    "\n",
    "        if not x_data:\n",
    "            # Skip this data point if x_data is empty\n",
    "            return self.__getitem__((idx + 1) % self.total_len)\n",
    "\n",
    "        # Convert x_data list to a single numpy array\n",
    "        x_data = np.array(x_data)\n",
    "\n",
    "        # Convert x_data to tensor\n",
    "        x_data = torch.tensor(x_data, dtype=torch.float32)\n",
    "\n",
    "        event_time = torch.tensor(event_time, dtype=torch.float32)\n",
    "        is_event = torch.tensor(is_event) \n",
    "\n",
    "        demo_feats = torch.tensor(demo_feats, dtype=torch.float32)\n",
    "\n",
    "        return x_data, event_time, is_event, demo_feats, self.max_channels, self.max_seq_len, hdf5_path\n",
    "\n",
    "\n",
    "def diagnosis_finetune_full_coxph_with_demo_collate_fn(batch):\n",
    "    x_data, event_time, is_event, demo_feats, max_channels_list, max_seq_len_list, hdf5_path_list = zip(*batch)\n",
    "\n",
    "    num_channels = max(max_channels_list)\n",
    "\n",
    "    if max_seq_len_list[0] == None:\n",
    "        max_seq_len = max([item.size(1) for item in x_data])\n",
    "    else:\n",
    "        max_seq_len = max_seq_len_list[0]\n",
    "\n",
    "    padded_x_data = []\n",
    "    padded_mask = []\n",
    "    for item in x_data:\n",
    "        c, s, e = item.size()\n",
    "        c = min(c, num_channels)\n",
    "        s = min(s, max_seq_len)  # Ensure the sequence length doesn't exceed max_seq_len\n",
    "\n",
    "        # Create a padded tensor and a mask tensor\n",
    "        padded_item = torch.zeros((num_channels, max_seq_len, e))\n",
    "        mask = torch.ones((num_channels, max_seq_len))\n",
    "\n",
    "        # Copy the actual data to the padded tensor and set the mask for real data\n",
    "        padded_item[:c, :s, :e] = item[:c, :s, :e]\n",
    "        mask[:c, :s] = 0  # 0 for real data, 1 for padding\n",
    "\n",
    "        padded_x_data.append(padded_item)\n",
    "        padded_mask.append(mask)\n",
    "    \n",
    "    # Stack all tensors into a batch\n",
    "    x_data = torch.stack(padded_x_data)\n",
    "    event_time = torch.stack(event_time)\n",
    "    is_event = torch.stack(is_event)\n",
    "    demo_feats = torch.stack(demo_feats)\n",
    "    padded_mask = torch.stack(padded_mask)\n",
    "    \n",
    "    return x_data, event_time, is_event, demo_feats, padded_mask, hdf5_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(base_save_path, \"demo_diagnosis\")\n",
    "os.makedirs(save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 3548.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in test set: 1\n",
      "Number of files to be processed in test set: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "hdf5_paths = [os.path.join(base_save_path, \"demo_emb/demo_psg.hdf5\")]\n",
    "demo_labels_path = os.path.join(base_save_path, \"demo_age_gender.csv\")\n",
    "config[\"labels_path\"] = base_save_path\n",
    "\n",
    "test_dataset = DiagnosisFinetuneFullCOXPHWithDemoDataset(config, channel_groups, split=\"test\", hdf5_paths=hdf5_paths, demo_labels_path=demo_labels_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=1, collate_fn=diagnosis_finetune_full_coxph_with_demo_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]/home/groups/jamesz/rthapa84/anaconda3/envs/sleepfm_clinical/lib/python3.10/site-packages/torch/nn/modules/transformer.py:562: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._transformer_encoder_layer_fwd(\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00,  3.42it/s]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "all_event_times = []\n",
    "all_is_event = []\n",
    "all_outputs = []\n",
    "all_paths = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for item in tqdm.tqdm(test_loader, desc=\"Evaluating\"):\n",
    "        x_data, event_times, is_event, demo_feats, padded_matrix, hdf5_path_list = item\n",
    "        x_data, event_times, is_event, demo_feats, padded_matrix, hdf5_path_list = x_data.to(device), event_times.to(device), is_event.to(device), demo_feats.to(device), padded_matrix.to(device), list(hdf5_path_list)\n",
    "        outputs = model(x_data, padded_matrix, demo_feats)\n",
    "    \n",
    "        logits = outputs.cpu().numpy()\n",
    "        all_outputs.append(logits)\n",
    "        all_event_times.append(event_times.cpu().numpy())\n",
    "        all_is_event.append(is_event.cpu().numpy())\n",
    "        all_paths.append(hdf5_path_list)\n",
    "\n",
    "all_outputs = np.concatenate(all_outputs, axis=0)\n",
    "all_event_times = np.concatenate(all_event_times, axis=0)\n",
    "all_is_event = np.concatenate(all_is_event, axis=0)\n",
    "all_paths = np.concatenate(all_paths)\n",
    "\n",
    "outputs_path = os.path.join(save_path, \"all_outputs.pickle\")\n",
    "event_times_path = os.path.join(save_path, \"all_event_times.pickle\")\n",
    "is_event_path = os.path.join(save_path, \"all_is_event.pickle\")\n",
    "file_paths = os.path.join(save_path, \"all_paths.pickle\")\n",
    "\n",
    "save_data(all_outputs, outputs_path)\n",
    "save_data(all_event_times, event_times_path)\n",
    "save_data(all_is_event, is_event_path)\n",
    "save_data(all_paths, file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 1065), (1, 1065), (1, 1065))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_outputs.shape, all_event_times.shape, all_is_event.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, you get the model outputs, which you can then use to look for specific disease diagnosis. Nope that the shape of the output above is 1065, meaning, this model gives logprobs for 1065 conditions. We provide information about each disease index and its corresponding phecode here `sleepfm/configs/label_mapping.csv`. You can map it as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = pd.read_csv(\"../sleepfm/configs/label_mapping.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df[\"output\"] = all_outputs[0]\n",
    "labels_df[\"is_event\"] = all_is_event[0]\n",
    "labels_df[\"event_time\"] = all_event_times[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_idx</th>\n",
       "      <th>phecode</th>\n",
       "      <th>phenotype</th>\n",
       "      <th>output</th>\n",
       "      <th>is_event</th>\n",
       "      <th>event_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Intestinal infection</td>\n",
       "      <td>2.381834</td>\n",
       "      <td>0</td>\n",
       "      <td>3845.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>8.5</td>\n",
       "      <td>Bacterial enteritis</td>\n",
       "      <td>2.941600</td>\n",
       "      <td>0</td>\n",
       "      <td>3845.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>8.6</td>\n",
       "      <td>Viral Enteritis</td>\n",
       "      <td>3.220508</td>\n",
       "      <td>0</td>\n",
       "      <td>3845.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>38.0</td>\n",
       "      <td>Septicemia</td>\n",
       "      <td>5.640359</td>\n",
       "      <td>0</td>\n",
       "      <td>3845.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>38.3</td>\n",
       "      <td>Bacteremia</td>\n",
       "      <td>5.597438</td>\n",
       "      <td>0</td>\n",
       "      <td>3845.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label_idx phecode             phenotype    output  is_event  event_time\n",
       "0          0     8.0  Intestinal infection  2.381834         0      3845.0\n",
       "1          1     8.5   Bacterial enteritis  2.941600         0      3845.0\n",
       "2          2     8.6       Viral Enteritis  3.220508         0      3845.0\n",
       "3          3    38.0            Septicemia  5.640359         0      3845.0\n",
       "4          4    38.3            Bacteremia  5.597438         0      3845.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, you get the output hazards from our model, and also your labels for is_event and event_times. Is_event is an indicator for if the event occured and event_time is the time to event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sleepfm_clinical",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
